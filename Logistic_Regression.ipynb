{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 1: What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "• **Logistic Regression** is a classification algorithm that predicts categorical outcomes (binary or multiclass) using a logistic function\n",
        "\n",
        "• **Linear Regression** predicts continuous numerical values, while Logistic Regression predicts probabilities and class labels\n",
        "\n",
        "• Logistic Regression uses the **sigmoid function** to map any real-valued number to a value between 0 and 1\n",
        "\n",
        "• **Output interpretation**: Linear regression gives direct values, logistic regression gives probabilities that need thresholding\n",
        "\n",
        "• **Use cases**: Linear regression for house prices, logistic regression for spam detection or customer churn prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 2: Explain the role of the Sigmoid function in Logistic Regression.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "• **Sigmoid function** (σ(z) = 1/(1 + e^(-z))) transforms linear combinations into probabilities between 0 and 1\n",
        "\n",
        "• **S-shaped curve** ensures smooth transitions and prevents extreme values, making it perfect for probability estimation\n",
        "\n",
        "• **Decision boundary**: The function creates a natural threshold at 0.5 for binary classification decisions\n",
        "\n",
        "• **Differentiability**: The sigmoid function is smooth and differentiable, enabling gradient descent optimization\n",
        "\n",
        "• **Interpretability**: Output values can be directly interpreted as probabilities, making the model more intuitive for business stakeholders\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 3: What is Regularization in Logistic Regression and why is it needed?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "• **Regularization** adds penalty terms to the cost function to prevent overfitting and improve generalization\n",
        "\n",
        "• **L1 (Lasso)**: Adds absolute value of coefficients as penalty, can drive coefficients to zero for feature selection\n",
        "\n",
        "• **L2 (Ridge)**: Adds squared coefficients as penalty, shrinks coefficients toward zero but keeps all features\n",
        "\n",
        "• **Prevents overfitting** by constraining model complexity, especially important with high-dimensional data\n",
        "\n",
        "• **Improves stability** by reducing variance in predictions and making the model more robust to noise in training data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 4: What are some common evaluation metrics for classification models, and why are they important?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "• **Accuracy**: Overall correctness, but can be misleading with imbalanced datasets\n",
        "\n",
        "• **Precision**: True positives / (True positives + False positives) - measures how many predicted positives are actually positive\n",
        "\n",
        "• **Recall (Sensitivity)**: True positives / (True positives + False negatives) - measures how many actual positives were correctly identified\n",
        "\n",
        "• **F1-Score**: Harmonic mean of precision and recall, balances both metrics for imbalanced datasets\n",
        "\n",
        "• **ROC-AUC**: Area under the ROC curve, measures model's ability to distinguish between classes across all thresholds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 5: Write a Python program that loads a CSV file into a Pandas DataFrame, splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.\n",
        "\n",
        "**Answer:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (569, 31)\n",
            "\n",
            "First few rows:\n",
            "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
            "0        17.99         10.38          122.80     1001.0          0.11840   \n",
            "1        20.57         17.77          132.90     1326.0          0.08474   \n",
            "2        19.69         21.25          130.00     1203.0          0.10960   \n",
            "3        11.42         20.38           77.58      386.1          0.14250   \n",
            "4        20.29         14.34          135.10     1297.0          0.10030   \n",
            "\n",
            "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
            "0           0.27760          0.3001              0.14710         0.2419   \n",
            "1           0.07864          0.0869              0.07017         0.1812   \n",
            "2           0.15990          0.1974              0.12790         0.2069   \n",
            "3           0.28390          0.2414              0.10520         0.2597   \n",
            "4           0.13280          0.1980              0.10430         0.1809   \n",
            "\n",
            "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
            "0                 0.07871  ...          17.33           184.60      2019.0   \n",
            "1                 0.05667  ...          23.41           158.80      1956.0   \n",
            "2                 0.05999  ...          25.53           152.50      1709.0   \n",
            "3                 0.09744  ...          26.50            98.87       567.7   \n",
            "4                 0.05883  ...          16.67           152.20      1575.0   \n",
            "\n",
            "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
            "0            0.1622             0.6656           0.7119                0.2654   \n",
            "1            0.1238             0.1866           0.2416                0.1860   \n",
            "2            0.1444             0.4245           0.4504                0.2430   \n",
            "3            0.2098             0.8663           0.6869                0.2575   \n",
            "4            0.1374             0.2050           0.4000                0.1625   \n",
            "\n",
            "   worst symmetry  worst fractal dimension  target  \n",
            "0          0.4601                  0.11890       0  \n",
            "1          0.2750                  0.08902       0  \n",
            "2          0.3613                  0.08758       0  \n",
            "3          0.6638                  0.17300       0  \n",
            "4          0.2364                  0.07678       0  \n",
            "\n",
            "[5 rows x 31 columns]\n",
            "\n",
            "Model Accuracy: 0.9561\n",
            "Training samples: 455\n",
            "Test samples: 114\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Abhi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
            "\n",
            "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
            "You might also want to scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Split features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 6: Write a Python program to train a Logistic Regression model using L2 regularization (Ridge) and print the model coefficients and accuracy.\n",
        "\n",
        "**Answer:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wine Dataset - Binary Classification\n",
            "Dataset shape: (178, 14)\n",
            "\n",
            "L2 Regularization Results:\n",
            "Accuracy: 1.0000\n",
            "Regularization strength (C): 1.0\n",
            "\n",
            "Model Coefficients:\n",
            "alcohol: 1.1018\n",
            "malic_acid: 0.5916\n",
            "ash: 0.9935\n",
            "alcalinity_of_ash: -0.3907\n",
            "magnesium: -0.0163\n",
            "total_phenols: 0.4541\n",
            "flavanoids: 1.3373\n",
            "nonflavanoid_phenols: 0.1403\n",
            "proanthocyanins: -0.1387\n",
            "color_intensity: -0.0583\n",
            "hue: 0.0314\n",
            "od280/od315_of_diluted_wines: 0.8082\n",
            "proline: 0.0129\n",
            "\n",
            "Intercept: -25.3053\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        35\n",
            "           1       1.00      1.00      1.00        19\n",
            "\n",
            "    accuracy                           1.00        54\n",
            "   macro avg       1.00      1.00      1.00        54\n",
            "weighted avg       1.00      1.00      1.00        54\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load wine dataset\n",
        "data = load_wine()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "print(\"Wine Dataset - Binary Classification\")\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "\n",
        "# Create binary classification (class 0 vs others)\n",
        "df['binary_target'] = (df['target'] == 0).astype(int)\n",
        "\n",
        "X = df.drop(['target', 'binary_target'], axis=1)\n",
        "y = df['binary_target']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression with L2 regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', C=1.0, random_state=42, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nL2 Regularization Results:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Regularization strength (C): {1.0}\")\n",
        "print(f\"\\nModel Coefficients:\")\n",
        "for i, coef in enumerate(model.coef_[0]):\n",
        "    print(f\"{X.columns[i]}: {coef:.4f}\")\n",
        "\n",
        "print(f\"\\nIntercept: {model.intercept_[0]:.4f}\")\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 7: Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr' and print the classification report.\n",
        "\n",
        "**Answer:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iris Dataset - Multiclass Classification\n",
            "Dataset shape: (150, 5)\n",
            "\n",
            "Target classes: ['setosa' 'versicolor' 'virginica']\n",
            "\n",
            "Class distribution:\n",
            "target\n",
            "0    50\n",
            "1    50\n",
            "2    50\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Multiclass Classification Results:\n",
            "Training samples: 112\n",
            "Test samples: 38\n",
            "\n",
            "Confusion Matrix:\n",
            "[[15  0  0]\n",
            " [ 0 10  1]\n",
            " [ 0  0 12]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        15\n",
            "  versicolor       1.00      0.91      0.95        11\n",
            "   virginica       0.92      1.00      0.96        12\n",
            "\n",
            "    accuracy                           0.97        38\n",
            "   macro avg       0.97      0.97      0.97        38\n",
            "weighted avg       0.98      0.97      0.97        38\n",
            "\n",
            "\n",
            "Model Coefficients for each class:\n",
            "\n",
            "setosa vs Rest:\n",
            "  sepal length (cm): -0.4150\n",
            "  sepal width (cm): 0.8674\n",
            "  petal length (cm): -2.1851\n",
            "  petal width (cm): -0.9055\n",
            "\n",
            "versicolor vs Rest:\n",
            "  sepal length (cm): -0.1534\n",
            "  sepal width (cm): -2.0957\n",
            "  petal length (cm): 0.5458\n",
            "  petal width (cm): -0.9761\n",
            "\n",
            "virginica vs Rest:\n",
            "  sepal length (cm): -0.3708\n",
            "  sepal width (cm): -0.5044\n",
            "  petal length (cm): 2.7271\n",
            "  petal width (cm): 2.0208\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Abhi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1273: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Load iris dataset for multiclass classification\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "print(\"Iris Dataset - Multiclass Classification\")\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nTarget classes:\", data.target_names)\n",
        "print(\"\\nClass distribution:\")\n",
        "print(df['target'].value_counts().sort_index())\n",
        "\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Train Logistic Regression with One-vs-Rest (OvR) strategy\n",
        "model = LogisticRegression(multi_class='ovr', random_state=42, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(f\"\\nMulticlass Classification Results:\")\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "\n",
        "print(\"\\nModel Coefficients for each class:\")\n",
        "for i, class_name in enumerate(data.target_names):\n",
        "    print(f\"\\n{class_name} vs Rest:\")\n",
        "    for j, feature in enumerate(data.feature_names):\n",
        "        print(f\"  {feature}: {model.coef_[i][j]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 8: Write a Python program to apply GridSearchCV to tune C and penalty hyperparameters for Logistic Regression and print the best parameters and validation accuracy.\n",
        "\n",
        "**Answer:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Breast Cancer Dataset - Hyperparameter Tuning\n",
            "Dataset shape: (569, 31)\n",
            "\n",
            "Starting Grid Search...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "\n",
            "Best Parameters: {'C': 100, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Best Cross-Validation Score: 0.9670\n",
            "Test Set Accuracy: 0.9825\n",
            "\n",
            "Detailed Results:\n",
            "   param_C param_penalty param_solver  mean_test_score  std_test_score\n",
            "0    0.001            l1    liblinear         0.914286        0.039560\n",
            "1    0.001            l1         saga         0.912088        0.041117\n",
            "2    0.001            l2    liblinear         0.920879        0.045786\n",
            "3    0.001            l2         saga         0.914286        0.040166\n",
            "4    0.010            l1    liblinear         0.914286        0.041931\n",
            "5    0.010            l1         saga         0.914286        0.040166\n",
            "6    0.010            l2    liblinear         0.925275        0.045786\n",
            "7    0.010            l2         saga         0.916484        0.044284\n",
            "8    0.100            l1    liblinear         0.920879        0.038945\n",
            "9    0.100            l1         saga         0.914286        0.044719\n",
            "\n",
            "Classification Report on Test Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.98      0.98      0.98        43\n",
            "      benign       0.99      0.99      0.99        71\n",
            "\n",
            "    accuracy                           0.98       114\n",
            "   macro avg       0.98      0.98      0.98       114\n",
            "weighted avg       0.98      0.98      0.98       114\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "print(\"Breast Cancer Dataset - Hyperparameter Tuning\")\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']  # solvers that support both L1 and L2\n",
        "}\n",
        "\n",
        "# Create base model\n",
        "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "# GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(\n",
        "    log_reg, \n",
        "    param_grid, \n",
        "    cv=5, \n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nStarting Grid Search...\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters and score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(f\"\\nBest Parameters: {best_params}\")\n",
        "print(f\"Best Cross-Validation Score: {best_score:.4f}\")\n",
        "\n",
        "# Test on holdout set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Test Set Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nDetailed Results:\")\n",
        "results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "print(results_df[['param_C', 'param_penalty', 'param_solver', 'mean_test_score', 'std_test_score']].head(10))\n",
        "\n",
        "print(\"\\nClassification Report on Test Set:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 9: Write a Python program to standardize the features before training Logistic Regression and compare the model's accuracy with and without scaling.\n",
        "\n",
        "**Answer:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wine Dataset - Feature Scaling Comparison\n",
            "Dataset shape: (178, 14)\n",
            "\n",
            "Feature statistics (before scaling):\n",
            "       alcohol  malic_acid     ash  alcalinity_of_ash  magnesium  \\\n",
            "count   178.00      178.00  178.00             178.00     178.00   \n",
            "mean     13.00        2.34    2.37              19.49      99.74   \n",
            "std       0.81        1.12    0.27               3.34      14.28   \n",
            "min      11.03        0.74    1.36              10.60      70.00   \n",
            "25%      12.36        1.60    2.21              17.20      88.00   \n",
            "50%      13.05        1.87    2.36              19.50      98.00   \n",
            "75%      13.68        3.08    2.56              21.50     107.00   \n",
            "max      14.83        5.80    3.23              30.00     162.00   \n",
            "\n",
            "       total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
            "count         178.00      178.00                178.00           178.00   \n",
            "mean            2.30        2.03                  0.36             1.59   \n",
            "std             0.63        1.00                  0.12             0.57   \n",
            "min             0.98        0.34                  0.13             0.41   \n",
            "25%             1.74        1.20                  0.27             1.25   \n",
            "50%             2.36        2.13                  0.34             1.56   \n",
            "75%             2.80        2.88                  0.44             1.95   \n",
            "max             3.88        5.08                  0.66             3.58   \n",
            "\n",
            "       color_intensity     hue  od280/od315_of_diluted_wines  proline  target  \n",
            "count           178.00  178.00                        178.00   178.00  178.00  \n",
            "mean              5.06    0.96                          2.61   746.89    0.94  \n",
            "std               2.32    0.23                          0.71   314.91    0.78  \n",
            "min               1.28    0.48                          1.27   278.00    0.00  \n",
            "25%               3.22    0.78                          1.94   500.50    0.00  \n",
            "50%               4.69    0.96                          2.78   673.50    1.00  \n",
            "75%               6.20    1.12                          3.17   985.00    2.00  \n",
            "max              13.00    1.71                          4.00  1680.00    2.00  \n",
            "\n",
            "============================================================\n",
            "COMPARISON: WITH vs WITHOUT FEATURE SCALING\n",
            "============================================================\n",
            "\n",
            "1. LOGISTIC REGRESSION WITHOUT SCALING:\n",
            "----------------------------------------\n",
            "Accuracy without scaling: 1.0000\n",
            "Convergence: No\n",
            "Iterations needed: 1000\n",
            "\n",
            "2. LOGISTIC REGRESSION WITH STANDARD SCALING:\n",
            "----------------------------------------\n",
            "\n",
            "Scaled feature statistics:\n",
            "Mean: [0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Std:  [1.0041, 1.0041, 1.0041, 1.0041, 1.0041, 1.0041, 1.0041, 1.0041, 1.0041, 1.0041, 1.0041, 1.0041, 1.0041]\n",
            "\n",
            "Accuracy with scaling: 0.9815\n",
            "Convergence: Yes\n",
            "Iterations needed: 16\n",
            "\n",
            "============================================================\n",
            "COMPARISON RESULTS:\n",
            "============================================================\n",
            "Accuracy without scaling: 1.0000\n",
            "Accuracy with scaling:    0.9815\n",
            "Improvement: -0.0185\n",
            "Iterations without scaling: 1000\n",
            "Iterations with scaling:    16\n",
            "\n",
            "Classification Report (With Scaling):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class_0       1.00      1.00      1.00        19\n",
            "     class_1       1.00      0.95      0.98        21\n",
            "     class_2       0.93      1.00      0.97        14\n",
            "\n",
            "    accuracy                           0.98        54\n",
            "   macro avg       0.98      0.98      0.98        54\n",
            "weighted avg       0.98      0.98      0.98        54\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Abhi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
            "\n",
            "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
            "You might also want to scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load wine dataset\n",
        "data = load_wine()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "print(\"Wine Dataset - Feature Scaling Comparison\")\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nFeature statistics (before scaling):\")\n",
        "print(df.describe().round(2))\n",
        "\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARISON: WITH vs WITHOUT FEATURE SCALING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. WITHOUT SCALING\n",
        "print(\"\\n1. LOGISTIC REGRESSION WITHOUT SCALING:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "model_no_scale = LogisticRegression(random_state=42, max_iter=1000)\n",
        "model_no_scale.fit(X_train, y_train)\n",
        "\n",
        "y_pred_no_scale = model_no_scale.predict(X_test)\n",
        "accuracy_no_scale = accuracy_score(y_test, y_pred_no_scale)\n",
        "\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scale:.4f}\")\n",
        "print(f\"Convergence: {'Yes' if model_no_scale.n_iter_[0] < 1000 else 'No'}\")\n",
        "print(f\"Iterations needed: {model_no_scale.n_iter_[0]}\")\n",
        "\n",
        "# 2. WITH SCALING\n",
        "print(\"\\n2. LOGISTIC REGRESSION WITH STANDARD SCALING:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"\\nScaled feature statistics:\")\n",
        "scaled_df = pd.DataFrame(X_train_scaled, columns=data.feature_names)\n",
        "print(f\"Mean: {scaled_df.mean().round(4).tolist()}\")\n",
        "print(f\"Std:  {scaled_df.std().round(4).tolist()}\")\n",
        "\n",
        "model_scaled = LogisticRegression(random_state=42, max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"\\nAccuracy with scaling: {accuracy_scaled:.4f}\")\n",
        "print(f\"Convergence: {'Yes' if model_scaled.n_iter_[0] < 1000 else 'No'}\")\n",
        "print(f\"Iterations needed: {model_scaled.n_iter_[0]}\")\n",
        "\n",
        "# COMPARISON\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARISON RESULTS:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scale:.4f}\")\n",
        "print(f\"Accuracy with scaling:    {accuracy_scaled:.4f}\")\n",
        "print(f\"Improvement: {accuracy_scaled - accuracy_no_scale:.4f}\")\n",
        "print(f\"Iterations without scaling: {model_no_scale.n_iter_[0]}\")\n",
        "print(f\"Iterations with scaling:    {model_scaled.n_iter_[0]}\")\n",
        "\n",
        "print(\"\\nClassification Report (With Scaling):\")\n",
        "print(classification_report(y_test, y_pred_scaled, target_names=data.target_names))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 10: Imagine you are working at an e-commerce company that wants to predict which customers will respond to a marketing campaign. Given an imbalanced dataset (only 5% of customers respond), describe the approach you'd take to build a Logistic Regression model — including data handling, feature scaling, balancing classes, hyperparameter tuning, and evaluating the model for this real-world business use case.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "• **Data Handling**: Clean missing values, engineer features like purchase frequency, recency, and monetary value, create customer segments based on behavior patterns\n",
        "\n",
        "• **Feature Scaling**: Apply StandardScaler to normalize numerical features since Logistic Regression is sensitive to scale, especially important for regularization\n",
        "\n",
        "• **Class Balancing**: Use SMOTE for oversampling minority class, or class_weight='balanced' parameter, or cost-sensitive learning to handle 5% response rate\n",
        "\n",
        "• **Hyperparameter Tuning**: GridSearchCV with different C values, penalty types (L1/L2), and class_weight options, use stratified CV to maintain class distribution\n",
        "\n",
        "• **Evaluation Strategy**: Focus on precision-recall curve and F1-score instead of accuracy, use business metrics like cost per acquisition and ROI, implement threshold tuning based on business costs\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
