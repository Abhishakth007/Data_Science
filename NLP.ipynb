{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTR9GQM_vWKT"
      },
      "source": [
        "# NLP Introduction & Text Processing - Assignment Answers\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1nf6vwkvWKc"
      },
      "source": [
        "## Question 1: What is Computational Linguistics and how does it relate to NLP?\n",
        "\n",
        "**Answer:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkuXRdpsvWKf"
      },
      "source": [
        "Computational Linguistics is an interdisciplinary field that combines linguistics, computer science, and artificial intelligence to study and model natural language using computational methods. It focuses on understanding the structure, meaning, and use of human language from a computational perspective.\n",
        "\n",
        "**Relationship with NLP:**\n",
        "\n",
        "Natural Language Processing (NLP) is the applied branch of Computational Linguistics. While Computational Linguistics is more research-oriented and theoretical, focusing on understanding how language works computationally, NLP is the practical application that builds systems and tools to process, understand, and generate human language.\n",
        "\n",
        "**Key Differences:**\n",
        "- **Computational Linguistics**: More theoretical, focuses on linguistic phenomena, language structure, and computational models of language\n",
        "- **NLP**: More applied, focuses on building practical systems like chatbots, translators, sentiment analyzers, etc.\n",
        "\n",
        "**Relationship**: NLP implements the theories and models developed in Computational Linguistics to create real-world applications. Computational Linguistics provides the foundation (linguistic theories, parsing algorithms, semantic models), while NLP applies these foundations to solve practical problems in industry and technology.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0-YSYYLvWKg"
      },
      "source": [
        "## Question 2: Briefly describe the historical evolution of Natural Language Processing.\n",
        "\n",
        "**Answer:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_Enb0ntvWKi"
      },
      "source": [
        "The historical evolution of Natural Language Processing can be divided into several key phases:\n",
        "\n",
        "**1. Early Period (1940s-1950s):**\n",
        "- Foundation laid by Alan Turing's work on machine intelligence\n",
        "- First machine translation experiments (Georgetown-IBM experiment, 1954)\n",
        "- Focus on rule-based systems and symbolic approaches\n",
        "\n",
        "**2. Symbolic Era (1960s-1980s):**\n",
        "- Development of formal grammars (Chomsky's transformational grammar)\n",
        "- Rule-based systems using hand-crafted linguistic rules\n",
        "- ELIZA chatbot (1966) demonstrated early conversational AI\n",
        "- Expert systems and knowledge bases for language understanding\n",
        "- Limited by the complexity of encoding all linguistic rules\n",
        "\n",
        "**3. Statistical Revolution (1990s-2000s):**\n",
        "- Shift from rule-based to statistical and probabilistic methods\n",
        "- Introduction of machine learning approaches\n",
        "- Hidden Markov Models (HMMs) for speech recognition\n",
        "- Statistical machine translation (IBM models)\n",
        "- N-gram language models\n",
        "- Part-of-speech tagging and parsing using statistical methods\n",
        "\n",
        "**4. Machine Learning Era (2000s-2010s):**\n",
        "- Support Vector Machines (SVMs) and Maximum Entropy models\n",
        "- Conditional Random Fields (CRFs) for sequence labeling\n",
        "- Feature engineering became crucial\n",
        "- Improved performance on various NLP tasks\n",
        "\n",
        "**5. Deep Learning Revolution (2010s-Present):**\n",
        "- Word embeddings (Word2Vec, GloVe) revolutionized representation learning\n",
        "- Recurrent Neural Networks (RNNs, LSTMs, GRUs) for sequence modeling\n",
        "- Convolutional Neural Networks (CNNs) for text classification\n",
        "- Attention mechanisms and Transformer architecture (2017)\n",
        "- Pre-trained language models (BERT, GPT, T5) achieving state-of-the-art results\n",
        "- Transfer learning and fine-tuning became standard practice\n",
        "\n",
        "**6. Large Language Models Era (2020s-Present):**\n",
        "- GPT-3, GPT-4, and other large-scale models\n",
        "- Few-shot and zero-shot learning capabilities\n",
        "- Multimodal models combining text, images, and other modalities\n",
        "- Democratization of NLP through accessible APIs and tools\n",
        "\n",
        "This evolution shows a progression from rigid rule-based systems to flexible, data-driven approaches that can learn patterns from large amounts of text data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBOCOKuIvWKj"
      },
      "source": [
        "## Question 3: List and explain three major use cases of NLP in today's tech industry.\n",
        "\n",
        "**Answer:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yffzoUbOvWKk"
      },
      "source": [
        "**1. Chatbots and Virtual Assistants:**\n",
        "- **Explanation**: NLP powers conversational AI systems that can understand user queries, maintain context, and provide appropriate responses. These systems use intent recognition, entity extraction, and dialogue management.\n",
        "- **Examples**: Customer service chatbots, virtual assistants (Siri, Alexa, Google Assistant), help desk automation\n",
        "- **Impact**: Reduces human workload, provides 24/7 support, handles routine queries efficiently\n",
        "\n",
        "**2. Sentiment Analysis and Opinion Mining:**\n",
        "- **Explanation**: NLP techniques analyze text to determine the emotional tone, sentiment (positive, negative, neutral), and opinions expressed. This involves text classification, aspect-based sentiment analysis, and emotion detection.\n",
        "- **Examples**: Social media monitoring, product review analysis, brand reputation management, market research\n",
        "- **Impact**: Helps businesses understand customer satisfaction, track brand perception, make data-driven decisions\n",
        "\n",
        "**3. Machine Translation and Language Services:**\n",
        "- **Explanation**: NLP enables automatic translation between languages using neural machine translation models. Modern systems use sequence-to-sequence models with attention mechanisms.\n",
        "- **Examples**: Google Translate, multilingual content localization, real-time translation apps, cross-language communication tools\n",
        "- **Impact**: Breaks down language barriers, enables global communication, facilitates international business\n",
        "\n",
        "**Additional Notable Use Cases:**\n",
        "- **Information Extraction**: Extracting structured data from unstructured text (named entity recognition, relation extraction)\n",
        "- **Text Summarization**: Automatic generation of concise summaries from long documents\n",
        "- **Search Engines**: Understanding search queries and retrieving relevant documents\n",
        "- **Email Filtering**: Spam detection and email categorization\n",
        "- **Content Recommendation**: Understanding user preferences from text data to recommend articles, products, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP2-9GeKvWKl"
      },
      "source": [
        "## Question 4: What is text normalization and why is it essential in text processing tasks?\n",
        "\n",
        "**Answer:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx-Ah-UOvWKm"
      },
      "source": [
        "**Text Normalization** is the process of converting text into a standard, consistent format by applying various transformations to handle variations, inconsistencies, and noise in raw text data.\n",
        "\n",
        "**Key Components of Text Normalization:**\n",
        "\n",
        "1. **Case Normalization**: Converting all text to lowercase (or uppercase) to ensure consistent representation\n",
        "   - Example: \"Hello\", \"HELLO\", \"hello\" → \"hello\"\n",
        "\n",
        "2. **Whitespace Normalization**: Standardizing spaces, tabs, and newlines\n",
        "   - Example: \"word1    word2\" → \"word1 word2\"\n",
        "\n",
        "3. **Punctuation Handling**: Removing or standardizing punctuation marks\n",
        "   - Example: \"Hello!\" → \"Hello\" or \"Hello !\"\n",
        "\n",
        "4. **Number Normalization**: Converting numbers to text or standardizing their representation\n",
        "   - Example: \"123\" → \"one hundred twenty three\" or \"<NUM>\"\n",
        "\n",
        "5. **Accent and Diacritic Removal**: Removing accents from characters\n",
        "   - Example: \"café\" → \"cafe\"\n",
        "\n",
        "6. **URL and Email Normalization**: Replacing URLs and emails with placeholders\n",
        "   - Example: \"Visit https://example.com\" → \"Visit <URL>\"\n",
        "\n",
        "7. **Contraction Expansion**: Expanding contractions\n",
        "   - Example: \"don't\" → \"do not\"\n",
        "\n",
        "**Why Text Normalization is Essential:**\n",
        "\n",
        "1. **Consistency**: Ensures uniform representation of text, reducing variations that could confuse models\n",
        "   - \"Hello\" and \"hello\" are treated as the same word\n",
        "\n",
        "2. **Improved Model Performance**: Machine learning models perform better with normalized data as they don't need to learn multiple representations of the same concept\n",
        "\n",
        "3. **Reduced Vocabulary Size**: Normalization reduces the vocabulary size, making models more efficient and reducing memory requirements\n",
        "\n",
        "4. **Better Tokenization**: Normalized text is easier to tokenize correctly, leading to better word boundaries and segmentation\n",
        "\n",
        "5. **Noise Reduction**: Removes irrelevant characters, formatting, and inconsistencies that don't contribute to meaning\n",
        "\n",
        "6. **Standardization Across Sources**: When processing text from multiple sources (social media, emails, documents), normalization ensures consistent format\n",
        "\n",
        "7. **Improved Search and Matching**: Normalized text enables better exact matching and similarity comparisons\n",
        "\n",
        "**Example Impact:**\n",
        "Without normalization: \"Hello\", \"HELLO\", \"hello\", \"Hello!\" might be treated as 4 different tokens\n",
        "With normalization: All become \"hello\" - treated as 1 token, improving model efficiency and accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc88xSvgvWKp"
      },
      "source": [
        "## Question 5: Compare and contrast stemming and lemmatization with suitable examples.\n",
        "\n",
        "**Answer:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWNfkQcvvWKp"
      },
      "source": [
        "**Stemming** and **Lemmatization** are both text normalization techniques used to reduce words to their base or root forms, but they differ in their approach and accuracy.\n",
        "\n",
        "### **Stemming:**\n",
        "\n",
        "**Definition**: Stemming is a heuristic process that chops off the end of words to get their root form (stem), often by removing suffixes using simple rules.\n",
        "\n",
        "**Characteristics:**\n",
        "- Fast and computationally inexpensive\n",
        "- Uses rule-based approach (e.g., remove \"-ing\", \"-ed\", \"-s\")\n",
        "- May produce stems that are not actual words\n",
        "- Doesn't consider context or part of speech\n",
        "- Can be aggressive and may over-stem or under-stem\n",
        "\n",
        "**Examples:**\n",
        "- \"running\" → \"run\"\n",
        "- \"flies\" → \"fli\" (not a real word)\n",
        "- \"better\" → \"better\" (doesn't reduce to \"good\")\n",
        "- \"studies\" → \"studi\"\n",
        "- \"happily\" → \"happili\"\n",
        "\n",
        "### **Lemmatization:**\n",
        "\n",
        "**Definition**: Lemmatization is a more sophisticated process that uses vocabulary and morphological analysis to return the base or dictionary form (lemma) of a word.\n",
        "\n",
        "**Characteristics:**\n",
        "- Slower and more computationally expensive\n",
        "- Uses dictionary lookup and morphological analysis\n",
        "- Always produces valid words\n",
        "- Considers context and part of speech (POS)\n",
        "- More accurate but requires POS tagging\n",
        "\n",
        "**Examples:**\n",
        "- \"running\" (verb) → \"run\"\n",
        "- \"flies\" (noun) → \"fly\"\n",
        "- \"better\" (adjective) → \"good\"\n",
        "- \"studies\" → \"study\"\n",
        "- \"happily\" → \"happy\"\n",
        "\n",
        "### **Comparison Table:**\n",
        "\n",
        "| Aspect | Stemming | Lemmatization |\n",
        "|--------|----------|---------------|\n",
        "| **Method** | Rule-based, heuristic | Dictionary-based, linguistic analysis |\n",
        "| **Speed** | Fast | Slower |\n",
        "| **Output** | May not be a real word | Always a valid word |\n",
        "| **Context Awareness** | No | Yes (requires POS) |\n",
        "| **Accuracy** | Lower | Higher |\n",
        "| **Example** | \"flies\" → \"fli\" | \"flies\" → \"fly\" |\n",
        "\n",
        "### **When to Use:**\n",
        "\n",
        "**Use Stemming when:**\n",
        "- Speed is critical\n",
        "- Approximate matching is acceptable\n",
        "- Working with large datasets where slight inaccuracy is tolerable\n",
        "- Building search engines or information retrieval systems\n",
        "\n",
        "**Use Lemmatization when:**\n",
        "- Accuracy is important\n",
        "- Need valid words for downstream tasks\n",
        "- Working with smaller datasets where quality matters\n",
        "- Building applications that require proper word forms (e.g., text generation, grammar checking)\n",
        "\n",
        "### **Practical Example:**\n",
        "\n",
        "**Input**: \"The cats are running and the dogs were barking happily.\"\n",
        "\n",
        "**Stemming Result**: \"the cat are run and the dog were bark happili\"\n",
        "\n",
        "**Lemmatization Result**: \"the cat be run and the dog be bark happy\"\n",
        "\n",
        "Note: Lemmatization correctly converts \"are\" → \"be\" and \"were\" → \"be\", while stemming doesn't handle these irregular forms.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6K0ZqWZzvWKq"
      },
      "source": [
        "## Question 6: Write a Python program that uses regular expressions (regex) to extract all email addresses from the following block of text.\n",
        "\n",
        "**Text:**\n",
        "\"Hello team, please contact us at support@xyz.com for technical issues, or reach out to our HR at hr@xyz.com.You can also connect with John at john.doe@xyz.org and jenny via jennyclarke126@mail.co.us. For partnership inquiries, email partners@xyz.biz.\"\n",
        "\n",
        "**Answer:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrvA16x1vWKr",
        "outputId": "8f614663-44e6-446e-e9f4-ec0aafc18658"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Email Addresses:\n",
            "--------------------------------------------------\n",
            "1. support@xyz.com\n",
            "2. hr@xyz.com.You\n",
            "3. john.doe@xyz.org\n",
            "4. jennyclarke126@mail.co.us\n",
            "5. partners@xyz.biz\n",
            "\n",
            "Total emails found: 5\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "text = \"Hello team, please contact us at support@xyz.com for technical issues, or reach out to our HR at hr@xyz.com.You can also connect with John at john.doe@xyz.org and jenny via jennyclarke126@mail.co.us. For partnership inquiries, email partners@xyz.biz.\"\n",
        "\n",
        "email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "\n",
        "\n",
        "emails = re.findall(email_pattern, text)\n",
        "\n",
        "\n",
        "print(\"Extracted Email Addresses:\")\n",
        "print(\"-\" * 50)\n",
        "for i, email in enumerate(emails, 1):\n",
        "    print(f\"{i}. {email}\")\n",
        "\n",
        "print(f\"\\nTotal emails found: {len(emails)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMBkEhjOvWKx"
      },
      "source": [
        "## Question 7: Given the sample paragraph below, perform string tokenization and frequency distribution using Python and NLTK.\n",
        "\n",
        "**Paragraph:**\n",
        "\"Natural Language Processing (NLP) is a fascinating field that combines linguistics, computer science, and artificial intelligence. It enables machines to understand, interpret, and generate human language. Applications of NLP include chatbots, sentiment analysis, and machine translation. As technology advances, the role of NLP in modern solutions is becoming increasingly critical.\"\n",
        "\n",
        "**Answer:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxZUHXd7vWKz",
        "outputId": "1f92f1aa-98c4-4a64-8224-492af8b0898d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Paragraph:\n",
            "--------------------------------------------------------------------------------\n",
            "Natural Language Processing (NLP) is a fascinating field that combines linguistics, computer science, and artificial intelligence. It enables machines to understand, interpret, and generate human language. Applications of NLP include chatbots, sentiment analysis, and machine translation. As technology advances, the role of NLP in modern solutions is becoming increasingly critical.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Tokenized Words:\n",
            "--------------------------------------------------------------------------------\n",
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'that', 'combines', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.', 'It', 'enables', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'Applications', 'of', 'NLP', 'include', 'chatbots', ',', 'sentiment', 'analysis', ',', 'and', 'machine', 'translation', '.', 'As', 'technology', 'advances', ',', 'the', 'role', 'of', 'NLP', 'in', 'modern', 'solutions', 'is', 'becoming', 'increasingly', 'critical', '.']\n",
            "\n",
            "Total tokens: 63\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Cleaned Tokens (lowercase, no punctuation):\n",
            "--------------------------------------------------------------------------------\n",
            "['natural', 'language', 'processing', 'nlp', 'is', 'a', 'fascinating', 'field', 'that', 'combines', 'linguistics', 'computer', 'science', 'and', 'artificial', 'intelligence', 'it', 'enables', 'machines', 'to', 'understand', 'interpret', 'and', 'generate', 'human', 'language', 'applications', 'of', 'nlp', 'include', 'chatbots', 'sentiment', 'analysis', 'and', 'machine', 'translation', 'as', 'technology', 'advances', 'the', 'role', 'of', 'nlp', 'in', 'modern', 'solutions', 'is', 'becoming', 'increasingly', 'critical']\n",
            "\n",
            "Total cleaned tokens: 50\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Frequency Distribution (Top 20 most common words):\n",
            "--------------------------------------------------------------------------------\n",
            "nlp                  :   3\n",
            "and                  :   3\n",
            "language             :   2\n",
            "is                   :   2\n",
            "of                   :   2\n",
            "natural              :   1\n",
            "processing           :   1\n",
            "a                    :   1\n",
            "fascinating          :   1\n",
            "field                :   1\n",
            "that                 :   1\n",
            "combines             :   1\n",
            "linguistics          :   1\n",
            "computer             :   1\n",
            "science              :   1\n",
            "artificial           :   1\n",
            "intelligence         :   1\n",
            "it                   :   1\n",
            "enables              :   1\n",
            "machines             :   1\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Frequency Distribution Statistics:\n",
            "--------------------------------------------------------------------------------\n",
            "Total unique words: 43\n",
            "Most common word: 'nlp' (appears 3 times)\n",
            "Total word count: 50\n",
            "\n",
            "================================================================================\n",
            "Complete Frequency Distribution (all words):\n",
            "--------------------------------------------------------------------------------\n",
            "and                  :   3\n",
            "nlp                  :   3\n",
            "is                   :   2\n",
            "language             :   2\n",
            "of                   :   2\n",
            "a                    :   1\n",
            "advances             :   1\n",
            "analysis             :   1\n",
            "applications         :   1\n",
            "artificial           :   1\n",
            "as                   :   1\n",
            "becoming             :   1\n",
            "chatbots             :   1\n",
            "combines             :   1\n",
            "computer             :   1\n",
            "critical             :   1\n",
            "enables              :   1\n",
            "fascinating          :   1\n",
            "field                :   1\n",
            "generate             :   1\n",
            "human                :   1\n",
            "in                   :   1\n",
            "include              :   1\n",
            "increasingly         :   1\n",
            "intelligence         :   1\n",
            "interpret            :   1\n",
            "it                   :   1\n",
            "linguistics          :   1\n",
            "machine              :   1\n",
            "machines             :   1\n",
            "modern               :   1\n",
            "natural              :   1\n",
            "processing           :   1\n",
            "role                 :   1\n",
            "science              :   1\n",
            "sentiment            :   1\n",
            "solutions            :   1\n",
            "technology           :   1\n",
            "that                 :   1\n",
            "the                  :   1\n",
            "to                   :   1\n",
            "translation          :   1\n",
            "understand           :   1\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    try:\n",
        "        nltk.download('punkt_tab', quiet=True)\n",
        "    except:\n",
        "        nltk.download('punkt', quiet=True)\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "\n",
        "\n",
        "paragraph = \"Natural Language Processing (NLP) is a fascinating field that combines linguistics, computer science, and artificial intelligence. It enables machines to understand, interpret, and generate human language. Applications of NLP include chatbots, sentiment analysis, and machine translation. As technology advances, the role of NLP in modern solutions is becoming increasingly critical.\"\n",
        "\n",
        "print(\"Original Paragraph:\")\n",
        "print(\"-\" * 80)\n",
        "print(paragraph)\n",
        "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "\n",
        "\n",
        "tokens = word_tokenize(paragraph)\n",
        "\n",
        "print(\"Tokenized Words:\")\n",
        "print(\"-\" * 80)\n",
        "print(tokens)\n",
        "print(f\"\\nTotal tokens: {len(tokens)}\")\n",
        "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "\n",
        "tokens_clean = [token.lower() for token in tokens if token not in string.punctuation]\n",
        "\n",
        "print(\"Cleaned Tokens (lowercase, no punctuation):\")\n",
        "print(\"-\" * 80)\n",
        "print(tokens_clean)\n",
        "print(f\"\\nTotal cleaned tokens: {len(tokens_clean)}\")\n",
        "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "\n",
        "freq_dist = FreqDist(tokens_clean)\n",
        "\n",
        "print(\"Frequency Distribution (Top 20 most common words):\")\n",
        "print(\"-\" * 80)\n",
        "for word, frequency in freq_dist.most_common(20):\n",
        "    print(f\"{word:20s} : {frequency:3d}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "\n",
        "print(\"Frequency Distribution Statistics:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"Total unique words: {len(freq_dist)}\")\n",
        "print(f\"Most common word: '{freq_dist.most_common(1)[0][0]}' (appears {freq_dist.most_common(1)[0][1]} times)\")\n",
        "print(f\"Total word count: {freq_dist.N()}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Complete Frequency Distribution (all words):\")\n",
        "print(\"-\" * 80)\n",
        "for word, frequency in sorted(freq_dist.items(), key=lambda x: (-x[1], x[0])):\n",
        "    print(f\"{word:20s} : {frequency:3d}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5ZmkbO-vWKz"
      },
      "source": [
        "## Question 8: Create a custom annotator using spaCy or NLTK that identifies and labels proper nouns in a given text.\n",
        "\n",
        "**Answer:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2AxO5EjvWK0",
        "outputId": "a3fc7651-fdfa-46a1-da80-c56129dc45b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "--------------------------------------------------------------------------------\n",
            "Apple Inc. was founded by Steve Jobs in Cupertino, California. Tim Cook is the current CEO. The company is known for products like iPhone, iPad, and MacBook. Microsoft Corporation, based in Redmond, Washington, is another major tech company.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Identified Proper Nouns:\n",
            "--------------------------------------------------------------------------------\n",
            "1. 'Apple' (Position: 0-5, Label: PROPN)\n",
            "2. 'Inc.' (Position: 6-10, Label: PROPN)\n",
            "3. 'Steve' (Position: 26-31, Label: PROPN)\n",
            "4. 'Jobs' (Position: 32-36, Label: PROPN)\n",
            "5. 'Cupertino' (Position: 40-49, Label: PROPN)\n",
            "6. 'California' (Position: 51-61, Label: PROPN)\n",
            "7. 'Tim' (Position: 63-66, Label: PROPN)\n",
            "8. 'Cook' (Position: 67-71, Label: PROPN)\n",
            "9. 'CEO' (Position: 87-90, Label: PROPN)\n",
            "10. 'iPhone' (Position: 131-137, Label: PROPN)\n",
            "11. 'iPad' (Position: 139-143, Label: PROPN)\n",
            "12. 'MacBook' (Position: 149-156, Label: PROPN)\n",
            "13. 'Microsoft' (Position: 158-167, Label: PROPN)\n",
            "14. 'Corporation' (Position: 168-179, Label: PROPN)\n",
            "15. 'Redmond' (Position: 190-197, Label: PROPN)\n",
            "16. 'Washington' (Position: 199-209, Label: PROPN)\n",
            "\n",
            "Total proper nouns found: 16\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Named Entity Recognition (NER) - More Detailed Analysis:\n",
            "--------------------------------------------------------------------------------\n",
            "Entity: 'Apple Inc.' | Label: ORG | Description: Companies, agencies, institutions, etc.\n",
            "Entity: 'Steve Jobs' | Label: PERSON | Description: People, including fictional\n",
            "Entity: 'Cupertino' | Label: GPE | Description: Countries, cities, states\n",
            "Entity: 'California' | Label: GPE | Description: Countries, cities, states\n",
            "Entity: 'Tim Cook' | Label: PERSON | Description: People, including fictional\n",
            "Entity: 'iPhone' | Label: ORG | Description: Companies, agencies, institutions, etc.\n",
            "Entity: 'iPad' | Label: ORG | Description: Companies, agencies, institutions, etc.\n",
            "Entity: 'MacBook' | Label: ORG | Description: Companies, agencies, institutions, etc.\n",
            "Entity: 'Microsoft Corporation' | Label: ORG | Description: Companies, agencies, institutions, etc.\n",
            "Entity: 'Redmond' | Label: GPE | Description: Countries, cities, states\n",
            "Entity: 'Washington' | Label: GPE | Description: Countries, cities, states\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Using Custom Annotator Function:\n",
            "--------------------------------------------------------------------------------\n",
            "Proper Noun: 'Apple' | POS: PROPN | Sentence: Apple Inc. was founded by Steve Jobs in Cupertino, Californi...\n",
            "Proper Noun: 'Inc.' | POS: PROPN | Sentence: Apple Inc. was founded by Steve Jobs in Cupertino, Californi...\n",
            "Proper Noun: 'Steve' | POS: PROPN | Sentence: Apple Inc. was founded by Steve Jobs in Cupertino, Californi...\n",
            "Proper Noun: 'Jobs' | POS: PROPN | Sentence: Apple Inc. was founded by Steve Jobs in Cupertino, Californi...\n",
            "Proper Noun: 'Cupertino' | POS: PROPN | Sentence: Apple Inc. was founded by Steve Jobs in Cupertino, Californi...\n",
            "Proper Noun: 'California' | POS: PROPN | Sentence: Apple Inc. was founded by Steve Jobs in Cupertino, Californi...\n",
            "Proper Noun: 'Tim' | POS: PROPN | Sentence: Tim Cook is the current CEO....\n",
            "Proper Noun: 'Cook' | POS: PROPN | Sentence: Tim Cook is the current CEO....\n",
            "Proper Noun: 'CEO' | POS: PROPN | Sentence: Tim Cook is the current CEO....\n",
            "Proper Noun: 'iPhone' | POS: PROPN | Sentence: The company is known for products like iPhone, iPad, and Mac...\n",
            "Proper Noun: 'iPad' | POS: PROPN | Sentence: The company is known for products like iPhone, iPad, and Mac...\n",
            "Proper Noun: 'MacBook' | POS: PROPN | Sentence: The company is known for products like iPhone, iPad, and Mac...\n",
            "Proper Noun: 'Microsoft' | POS: PROPN | Sentence: Microsoft Corporation, based in Redmond, Washington, is anot...\n",
            "Proper Noun: 'Corporation' | POS: PROPN | Sentence: Microsoft Corporation, based in Redmond, Washington, is anot...\n",
            "Proper Noun: 'Redmond' | POS: PROPN | Sentence: Microsoft Corporation, based in Redmond, Washington, is anot...\n",
            "Proper Noun: 'Washington' | POS: PROPN | Sentence: Microsoft Corporation, based in Redmond, Washington, is anot...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "try:\n",
        "    import spacy\n",
        "    USE_SPACY = True\n",
        "\n",
        "    try:\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "    except OSError:\n",
        "        print(\"spaCy English model not found. Falling back to NLTK.\")\n",
        "        print(\"To use spaCy, install it using: python -m spacy download en_core_web_sm\")\n",
        "        USE_SPACY = False\n",
        "except ImportError:\n",
        "    print(\"spaCy not installed. Using NLTK instead.\")\n",
        "    print(\"To install spaCy: pip install spacy\")\n",
        "    USE_SPACY = False\n",
        "\n",
        "\n",
        "text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California. Tim Cook is the current CEO. The company is known for products like iPhone, iPad, and MacBook. Microsoft Corporation, based in Redmond, Washington, is another major tech company.\"\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(\"-\" * 80)\n",
        "print(text)\n",
        "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "\n",
        "if USE_SPACY:\n",
        "\n",
        "    doc = nlp(text)\n",
        "\n",
        "    proper_nouns = []\n",
        "    for token in doc:\n",
        "        if token.pos_ == \"PROPN\":\n",
        "            proper_nouns.append({\n",
        "                'text': token.text,\n",
        "                'label': token.pos_,\n",
        "                'start': token.idx,\n",
        "                'end': token.idx + len(token.text)\n",
        "            })\n",
        "\n",
        "    print(\"Identified Proper Nouns:\")\n",
        "    print(\"-\" * 80)\n",
        "    for i, pn in enumerate(proper_nouns, 1):\n",
        "        print(f\"{i}. '{pn['text']}' (Position: {pn['start']}-{pn['end']}, Label: {pn['label']})\")\n",
        "\n",
        "    print(f\"\\nTotal proper nouns found: {len(proper_nouns)}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "\n",
        "    print(\"Named Entity Recognition (NER) - More Detailed Analysis:\")\n",
        "    print(\"-\" * 80)\n",
        "    for ent in doc.ents:\n",
        "        print(f\"Entity: '{ent.text}' | Label: {ent.label_} | Description: {spacy.explain(ent.label_)}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "\n",
        "    def identify_proper_nouns(text, model=nlp):\n",
        "        \"\"\"\n",
        "        Custom function to identify and label proper nouns in text.\n",
        "\n",
        "        Args:\n",
        "            text: Input text string\n",
        "            model: spaCy language model (default: en_core_web_sm)\n",
        "\n",
        "        Returns:\n",
        "            List of dictionaries containing proper noun information\n",
        "        \"\"\"\n",
        "        doc = model(text)\n",
        "        proper_nouns = []\n",
        "\n",
        "        for token in doc:\n",
        "            if token.pos_ == \"PROPN\":\n",
        "                proper_nouns.append({\n",
        "                    'word': token.text,\n",
        "                    'pos_tag': token.pos_,\n",
        "                    'pos_description': 'Proper Noun',\n",
        "                    'start_char': token.idx,\n",
        "                    'end_char': token.idx + len(token.text),\n",
        "                    'sentence': token.sent.text.strip()\n",
        "                })\n",
        "\n",
        "        return proper_nouns\n",
        "\n",
        "    print(\"Using Custom Annotator Function:\")\n",
        "    print(\"-\" * 80)\n",
        "    result = identify_proper_nouns(text)\n",
        "    for item in result:\n",
        "        print(f\"Proper Noun: '{item['word']}' | POS: {item['pos_tag']} | Sentence: {item['sentence'][:60]}...\")\n",
        "\n",
        "else:\n",
        "    import nltk\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    from nltk.tag import pos_tag\n",
        "\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt_tab')\n",
        "    except LookupError:\n",
        "        try:\n",
        "            nltk.download('punkt_tab', quiet=True)\n",
        "        except:\n",
        "            nltk.download('punkt', quiet=True)\n",
        "\n",
        "    try:\n",
        "        nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "    except LookupError:\n",
        "        nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged = pos_tag(tokens)\n",
        "\n",
        "    proper_nouns = []\n",
        "    for word, tag in tagged:\n",
        "        if tag in ['NNP', 'NNPS']:\n",
        "            proper_nouns.append({\n",
        "                'text': word,\n",
        "                'label': tag,\n",
        "                'description': 'Proper Noun (singular)' if tag == 'NNP' else 'Proper Noun (plural)'\n",
        "            })\n",
        "\n",
        "    print(\"Identified Proper Nouns (using NLTK):\")\n",
        "    print(\"-\" * 80)\n",
        "    for i, pn in enumerate(proper_nouns, 1):\n",
        "        print(f\"{i}. '{pn['text']}' (Label: {pn['label']}, {pn['description']})\")\n",
        "\n",
        "    print(f\"\\nTotal proper nouns found: {len(proper_nouns)}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "\n",
        "\n",
        "    def identify_proper_nouns(text):\n",
        "        \"\"\"\n",
        "        Custom function to identify and label proper nouns in text using NLTK.\n",
        "\n",
        "        Args:\n",
        "            text: Input text string\n",
        "\n",
        "        Returns:\n",
        "            List of dictionaries containing proper noun information\n",
        "        \"\"\"\n",
        "        tokens = word_tokenize(text)\n",
        "        tagged = pos_tag(tokens)\n",
        "\n",
        "        proper_nouns = []\n",
        "        for word, tag in tagged:\n",
        "            if tag in ['NNP', 'NNPS']:\n",
        "                proper_nouns.append({\n",
        "                    'word': word,\n",
        "                    'pos_tag': tag,\n",
        "                    'pos_description': 'Proper Noun (singular)' if tag == 'NNP' else 'Proper Noun (plural)'\n",
        "                })\n",
        "\n",
        "        return proper_nouns\n",
        "\n",
        "    # Test the custom annotator\n",
        "    print(\"Using Custom Annotator Function (NLTK):\")\n",
        "    print(\"-\" * 80)\n",
        "    result = identify_proper_nouns(text)\n",
        "    for item in result:\n",
        "        print(f\"Proper Noun: '{item['word']}' | POS: {item['pos_tag']} | {item['pos_description']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SB4IH-wvWK2"
      },
      "source": [
        "## Question 9: Using Gensim, demonstrate how to train a simple Word2Vec model on the following dataset.\n",
        "\n",
        "**Dataset:**\n",
        "- \"Natural language processing enables computers to understand human language\"\n",
        "- \"Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\"\n",
        "- \"Word2Vec is a popular word embedding technique used in many NLP applications\"\n",
        "- \"Text preprocessing is a critical step before training word embeddings\"\n",
        "- \"Tokenization and normalization help clean raw text for modeling\"\n",
        "\n",
        "**Answer:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vASfskrvWK2",
        "outputId": "affdaa91-5035-4e48-e5de-6b020aaf4f4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "Original Dataset:\n",
            "--------------------------------------------------------------------------------\n",
            "1. Natural language processing enables computers to understand human language\n",
            "2. Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\n",
            "3. Word2Vec is a popular word embedding technique used in many NLP applications\n",
            "4. Text preprocessing is a critical step before training word embeddings\n",
            "5. Tokenization and normalization help clean raw text for modeling\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Preprocessed Data (Tokenized and Lowercased):\n",
            "--------------------------------------------------------------------------------\n",
            "1. ['natural', 'language', 'processing', 'enables', 'computers', 'to', 'understand', 'human', 'language']\n",
            "2. ['word', 'embeddings', 'are', 'a', 'type', 'of', 'word', 'representation', 'that', 'allows', 'words', 'with', 'similar', 'meaning', 'to', 'have', 'similar', 'representation']\n",
            "3. ['is', 'a', 'popular', 'word', 'embedding', 'technique', 'used', 'in', 'many', 'nlp', 'applications']\n",
            "4. ['text', 'preprocessing', 'is', 'a', 'critical', 'step', 'before', 'training', 'word', 'embeddings']\n",
            "5. ['tokenization', 'and', 'normalization', 'help', 'clean', 'raw', 'text', 'for', 'modeling']\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Training Word2Vec Model...\n",
            "--------------------------------------------------------------------------------\n",
            "Model training completed!\n",
            "Vocabulary size: 45 unique words\n",
            "Total word tokens in dataset: 57\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Vocabulary:\n",
            "--------------------------------------------------------------------------------\n",
            "Words in vocabulary (45):\n",
            "['word', 'a', 'text', 'is', 'similar', 'representation', 'embeddings', 'to', 'language', 'modeling', 'for', 'raw', 'clean', 'help', 'normalization', 'and', 'tokenization', 'training', 'before', 'step', 'critical', 'preprocessing', 'applications', 'nlp', 'many', 'in', 'used', 'technique', 'embedding', 'popular', 'have', 'meaning', 'with', 'words', 'allows', 'that', 'of', 'type', 'are', 'human', 'understand', 'computers', 'enables', 'processing', 'natural']\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Word Vector Examples:\n",
            "--------------------------------------------------------------------------------\n",
            "'natural': Vector shape = (100,), First 5 values = [-0.00821512  0.00016764  0.00602279  0.00957354 -0.00284308]\n",
            "'language': Vector shape = (100,), First 5 values = [-0.00957823  0.00894396  0.00416303  0.0092366   0.00664241]\n",
            "'processing': Vector shape = (100,), First 5 values = [-0.00260359 -0.00792151 -0.00826411 -0.00758553  0.00355419]\n",
            "'word': Vector shape = (100,), First 5 values = [-0.00053519  0.00023352  0.00509937  0.00900648 -0.00930311]\n",
            "'embeddings': Vector shape = (100,), First 5 values = [ 0.00813738 -0.0044638  -0.00107052  0.00100596 -0.00019304]\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Finding Similar Words:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Words similar to 'language':\n",
            "  - technique: 0.2853\n",
            "  - text: 0.1991\n",
            "  - natural: 0.1902\n",
            "  - human: 0.1004\n",
            "  - step: 0.0968\n",
            "\n",
            "Words similar to 'word':\n",
            "  - tokenization: 0.2189\n",
            "  - modeling: 0.2163\n",
            "  - embedding: 0.1955\n",
            "  - processing: 0.1693\n",
            "  - have: 0.1517\n",
            "\n",
            "Words similar to 'processing':\n",
            "  - for: 0.2685\n",
            "  - allows: 0.2057\n",
            "  - with: 0.1983\n",
            "  - normalization: 0.1901\n",
            "  - word: 0.1693\n",
            "\n",
            "Words similar to 'text':\n",
            "  - computers: 0.2120\n",
            "  - language: 0.1991\n",
            "  - raw: 0.1727\n",
            "  - popular: 0.1712\n",
            "  - similar: 0.1702\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Word Similarity Examples:\n",
            "--------------------------------------------------------------------------------\n",
            "Similarity between 'natural' and 'language': 0.1902\n",
            "Similarity between 'word' and 'embeddings': 0.0163\n",
            "Similarity between 'text' and 'processing': -0.1532\n",
            "Similarity between 'tokenization' and 'normalization': 0.0492\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Word2Vec Model Training and Demonstration Complete!\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.word2vec import LineSentence\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    try:\n",
        "        nltk.download('punkt_tab', quiet=True)\n",
        "    except:\n",
        "        nltk.download('punkt', quiet=True)\n",
        "\n",
        "# Dataset\n",
        "dataset = [\n",
        "    \"Natural language processing enables computers to understand human language\",\n",
        "    \"Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\",\n",
        "    \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        "    \"Text preprocessing is a critical step before training word embeddings\",\n",
        "    \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "\n",
        "print(\"Original Dataset:\")\n",
        "print(\"-\" * 80)\n",
        "for i, sentence in enumerate(dataset, 1):\n",
        "    print(f\"{i}. {sentence}\")\n",
        "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess text: tokenize, lowercase, remove punctuation\n",
        "    \"\"\"\n",
        "\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token.isalpha()]\n",
        "    return tokens\n",
        "\n",
        "processed_data = [preprocess_text(sentence) for sentence in dataset]\n",
        "\n",
        "print(\"Preprocessed Data (Tokenized and Lowercased):\")\n",
        "print(\"-\" * 80)\n",
        "for i, tokens in enumerate(processed_data, 1):\n",
        "    print(f\"{i}. {tokens}\")\n",
        "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "\n",
        "print(\"Training Word2Vec Model...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "\n",
        "model = Word2Vec(\n",
        "    sentences=processed_data,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    workers=1,\n",
        "    sg=0\n",
        ")\n",
        "\n",
        "print(\"Model training completed!\")\n",
        "print(f\"Vocabulary size: {len(model.wv.key_to_index)} unique words\")\n",
        "\n",
        "total_word_count = sum(len(sentence) for sentence in processed_data)\n",
        "print(f\"Total word tokens in dataset: {total_word_count}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "\n",
        "print(\"Vocabulary:\")\n",
        "print(\"-\" * 80)\n",
        "vocab = list(model.wv.key_to_index.keys())\n",
        "print(f\"Words in vocabulary ({len(vocab)}):\")\n",
        "print(vocab)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "\n",
        "\n",
        "print(\"Word Vector Examples:\")\n",
        "print(\"-\" * 80)\n",
        "sample_words = ['natural', 'language', 'processing', 'word', 'embeddings']\n",
        "for word in sample_words:\n",
        "    if word in model.wv:\n",
        "        vector = model.wv[word]\n",
        "        print(f\"'{word}': Vector shape = {vector.shape}, First 5 values = {vector[:5]}\")\n",
        "    else:\n",
        "        print(f\"'{word}': Not in vocabulary\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "\n",
        "print(\"Finding Similar Words:\")\n",
        "print(\"-\" * 80)\n",
        "test_words = ['language', 'word', 'processing', 'text']\n",
        "for word in test_words:\n",
        "    if word in model.wv:\n",
        "        similar = model.wv.most_similar(word, topn=5)\n",
        "        print(f\"\\nWords similar to '{word}':\")\n",
        "        for similar_word, similarity in similar:\n",
        "            print(f\"  - {similar_word}: {similarity:.4f}\")\n",
        "    else:\n",
        "        print(f\"'{word}' not in vocabulary\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "\n",
        "print(\"Word Similarity Examples:\")\n",
        "print(\"-\" * 80)\n",
        "word_pairs = [\n",
        "    ('natural', 'language'),\n",
        "    ('word', 'embeddings'),\n",
        "    ('text', 'processing'),\n",
        "    ('tokenization', 'normalization')\n",
        "]\n",
        "\n",
        "for word1, word2 in word_pairs:\n",
        "    if word1 in model.wv and word2 in model.wv:\n",
        "        similarity = model.wv.similarity(word1, word2)\n",
        "        print(f\"Similarity between '{word1}' and '{word2}': {similarity:.4f}\")\n",
        "    else:\n",
        "        missing = [w for w in [word1, word2] if w not in model.wv]\n",
        "        print(f\"Cannot compute similarity: {missing} not in vocabulary\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "\n",
        "print(\"Word2Vec Model Training and Demonstration Complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzlmQr3CvWK3"
      },
      "source": [
        "## Question 10: Imagine you are a data scientist at a fintech startup. You've been tasked with analyzing customer feedback. Outline the steps you would take to clean, process, and extract useful insights using NLP techniques from thousands of customer reviews.\n",
        "\n",
        "**Answer:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning & Normalization\n",
        "Remove duplicates, fix encoding issues, strip HTML, standardize casing, correct obvious misspellings, and clean emojis/special characters depending on use-case.\n",
        "\n",
        "Text Preprocessing\n",
        "Tokenize → remove stopwords → lemmatize/stem → handle negations → detect language and filter non-English reviews.\n",
        "\n",
        "Exploratory Text Analysis\n",
        "Compute word frequencies, n-grams, keyword extraction (TF-IDF), and identify recurring themes to understand dominant topics and issues.\n",
        "\n",
        "Sentiment & Emotion Classification\n",
        "Use pre-trained transformers or fine-tuned models (e.g., BERT, DistilBERT) to assign sentiment scores and detect emotions like anger, trust, confusion.\n",
        "\n",
        "Topic Modeling / Clustering\n",
        "Apply LDA, BERTopic, or sentence-embedding + clustering to discover underlying categories (e.g., “app crashes,” “payment delays,” “customer service”).\n",
        "\n",
        "Insight Extraction & Summarization\n",
        "Generate human-readable summaries per topic, extract frequently mentioned entities (NER), and identify pain points/feature requests.\n",
        "\n",
        "Dashboards & Business Reporting\n",
        "Build visual dashboards showing trends, sentiment over time, top complaints, and actionable recommendations for product or support teams."
      ],
      "metadata": {
        "id": "7K-RfUiyxDzi"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}